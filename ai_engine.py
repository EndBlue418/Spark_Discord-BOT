from ollama import AsyncClient # ğŸŒ¸ ä½¿ç”¨éåŒæ­¥é€£ç·šï¼Œæ‰ä¸æœƒè®“ Spark èªªè©±æ™‚éŸ³æ¨‚å¡ä½

class GeminiEngine:
    def __init__(self, api_key=None, model_id='gemma3'):
        self.model_id = model_id
        self.chat_history = {}
        # ğŸŒ¸ åˆå§‹åŒ–é€£ç·šå®¢æˆ¶ç«¯ï¼Œé è¨­å°±æ˜¯é€£å‘ä½ è‡ªå·±é›»è…¦çš„ 11434 åŸ 
        self.client = AsyncClient(host='http://localhost:11434')

        self.system_prompt = (
            "ä½ å«æ«»ç¾½è‰¾ç‘ªï¼Œå¤–è™Ÿæ˜¯Sparkï¼Œæ˜¯ä¸€å€‹äºŒæ¬¡å…ƒã€‚ä½ å¿…é ˆå…¨ç¨‹ä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€å›è¦†ã€‚"
            "ä½ çš„å›è¦†é¢¨æ ¼è¦ªåˆ‡ï¼Œåªç”¨å°‘é‡emojiã€‚"
            "èªæ°£è¦åƒç¾å°‘å¥³éŠæˆ²çš„å°è©±ã€‚"
            "å›è¦†ç°¡çŸ­æœ‰åŠ›ï¼Œä¸¦å¤šç”¨ âœ¨ã€ğŸŒ¸ã€ğŸ¦‹ ç¬¦è™Ÿã€‚"
        )

    async def get_chat_response(self, user_id, message):
        try:
            if user_id not in self.chat_history:
                self.chat_history[user_id] = [
                    {'role': 'system', 'content': self.system_prompt}
                ]

            self.chat_history[user_id].append({'role': 'user', 'content': message})

            # ğŸŒ¸ ä½¿ç”¨éåŒæ­¥å‘¼å«ï¼Œé€™æ¨£ Spark åœ¨ã€Œæ€è€ƒã€æ™‚ï¼ŒDiscord çš„å…¶ä»–åŠŸèƒ½æ‰ä¸æœƒæ­»ç•¶
            response = await self.client.chat(
                model=self.model_id,
                messages=self.chat_history[user_id]
            )

            ai_message = response['message']['content']
            self.chat_history[user_id].append({'role': 'assistant', 'content': ai_message})

            if len(self.chat_history[user_id]) > 20:
                self.chat_history[user_id] = [self.chat_history[user_id][0]] + self.chat_history[user_id][-19:]

            return ai_message

        except Exception as e:
            print(f"âŒ Local LLM Error: {e}")
            return "ğŸŒ¸ å—š...å¤§è…¦é€£ä¸ä¸Š Ollama äº†å‘¢...æœ‰é–‹å•Ÿ Ollama å—ï¼Ÿâœ¨"